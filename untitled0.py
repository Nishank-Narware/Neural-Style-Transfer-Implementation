# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YD5XiHCfY4rAMIXcJ-ELVWLCPmrrJqw5
"""

#!/usr/bin/env python3
"""
Neural Style Transfer Implementation
CODTECH Internship Project

This script implements Neural Style Transfer using TensorFlow/Keras
to apply artistic styles to photographs using a pre-trained VGG19 model.

Author: CODTECH Intern
Date: 2025
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import os
import time
from tensorflow.keras.applications import vgg19
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.vgg19 import preprocess_input

# Disable GPU warnings (optional)
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

class NeuralStyleTransfer:
    """
    Neural Style Transfer implementation using VGG19 pre-trained model
    """

    def __init__(self, img_height=512, img_width=512):
        """
        Initialize the Neural Style Transfer model

        Args:
            img_height (int): Target image height
            img_width (int): Target image width
        """
        self.img_height = img_height
        self.img_width = img_width

        # Content layer where we will pull our feature maps
        self.content_layers = ['block5_conv2']

        # Style layers of interest
        self.style_layers = ['block1_conv1',
                            'block2_conv1',
                            'block3_conv1',
                            'block4_conv1',
                            'block5_conv1']

        self.num_content_layers = len(self.content_layers)
        self.num_style_layers = len(self.style_layers)

        # Build the model
        self.model = self.build_model()

    def load_img(self, path_to_img):
        """
        Load and preprocess image

        Args:
            path_to_img (str): Path to image file

        Returns:
            tf.Tensor: Preprocessed image tensor
        """
        max_dim = max(self.img_height, self.img_width)
        img = tf.io.read_file(path_to_img)
        img = tf.image.decode_image(img, channels=3)
        img = tf.image.convert_image_dtype(img, tf.float32)

        shape = tf.cast(tf.shape(img)[:-1], tf.float32)
        long_dim = max(shape)
        scale = max_dim / long_dim

        new_shape = tf.cast(shape * scale, tf.int32)

        img = tf.image.resize(img, new_shape)
        img = img[tf.newaxis, :]
        return img

    def load_img_from_array(self, img_array):
        """
        Load image from numpy array

        Args:
            img_array (np.array): Image as numpy array

        Returns:
            tf.Tensor: Preprocessed image tensor
        """
        img = tf.convert_to_tensor(img_array, dtype=tf.float32)
        if len(img.shape) == 3:
            img = img[tf.newaxis, :]

        img = tf.image.resize(img, [self.img_height, self.img_width])
        return img / 255.0

    def imshow(self, image, title=None):
        """
        Display image

        Args:
            image (tf.Tensor): Image tensor
            title (str): Title for the plot
        """
        if len(image.shape) > 3:
            image = tf.squeeze(image, axis=0)

        plt.figure(figsize=(10, 10))
        plt.imshow(image)
        if title:
            plt.title(title)
        plt.axis('off')
        plt.show()

    def build_model(self):
        """
        Build the VGG model for feature extraction

        Returns:
            tf.keras.Model: Feature extraction model
        """
        # Load VGG19 model
        vgg = vgg19.VGG19(include_top=False, weights='imagenet')
        vgg.trainable = False

        # Get output layers corresponding to style and content layers
        style_outputs = [vgg.get_layer(name).output for name in self.style_layers]
        content_outputs = [vgg.get_layer(name).output for name in self.content_layers]
        model_outputs = style_outputs + content_outputs

        # Build model
        return tf.keras.models.Model(vgg.input, model_outputs)

    def get_content_loss(self, base_content, target):
        """
        Calculate content loss

        Args:
            base_content (tf.Tensor): Content features from original image
            target (tf.Tensor): Content features from generated image

        Returns:
            tf.Tensor: Content loss
        """
        return tf.reduce_mean(tf.square(base_content - target))

    def gram_matrix(self, input_tensor):
        """
        Calculate Gram matrix for style representation

        Args:
            input_tensor (tf.Tensor): Feature maps

        Returns:
            tf.Tensor: Gram matrix
        """
        # Ensure input_tensor has batch dimension
        if len(input_tensor.shape) == 3:
            input_tensor = tf.expand_dims(input_tensor, 0)

        # Get dimensions
        input_shape = tf.shape(input_tensor)
        batch_size = input_shape[0]
        height = input_shape[1]
        width = input_shape[2]
        channels = input_shape[3]

        # Reshape to (batch_size, height*width, channels)
        features = tf.reshape(input_tensor, [batch_size, height * width, channels])

        # Calculate Gram matrix: features^T * features
        gram = tf.matmul(features, features, transpose_a=True)

        # Normalize by number of locations
        num_locations = tf.cast(height * width, tf.float32)
        return gram / num_locations

    def get_style_loss(self, base_style, gram_target):
        """
        Calculate style loss

        Args:
            base_style (tf.Tensor): Style features from style image
            gram_target (tf.Tensor): Style features from generated image

        Returns:
            tf.Tensor: Style loss
        """
        gram_style = self.gram_matrix(base_style)
        return tf.reduce_mean(tf.square(gram_style - gram_target))

    def get_feature_representations(self, model, content_path, style_path):
        """
        Get feature representations for content and style images

        Args:
            model: Feature extraction model
            content_path (str): Path to content image
            style_path (str): Path to style image

        Returns:
            tuple: Style and content features
        """
        # Load images
        content_image = self.load_img(content_path)
        style_image = self.load_img(style_path)

        # Batch and preprocess for VGG19
        content_image = tf.cast(content_image * 255.0, tf.float32)
        style_image = tf.cast(style_image * 255.0, tf.float32)

        # Preprocess for VGG19
        content_image = vgg19.preprocess_input(content_image)
        style_image = vgg19.preprocess_input(style_image)

        # Get features
        style_outputs = model(style_image)
        content_outputs = model(content_image)

        # Get the style and content feature representations from our model
        style_features = [style_layer for style_layer in style_outputs[:self.num_style_layers]]
        content_features = [content_layer for content_layer in content_outputs[self.num_style_layers:]]

        return style_features, content_features

    def compute_loss(self, model, loss_weights, init_image, gram_style_features, content_features):
        """
        Compute total loss

        Args:
            model: Feature extraction model
            loss_weights (tuple): Weights for style and content loss
            init_image (tf.Tensor): Generated image
            gram_style_features (list): Style features
            content_features (list): Content features

        Returns:
            tuple: Total loss, style loss, content loss
        """
        style_weight, content_weight = loss_weights

        # Preprocess the generated image for VGG19
        preprocessed_image = vgg19.preprocess_input(init_image * 255.0)

        # Feed the generated image through the model
        model_outputs = model(preprocessed_image)

        style_output_features = model_outputs[:self.num_style_layers]
        content_output_features = model_outputs[self.num_style_layers:]

        style_score = 0
        content_score = 0

        # Accumulate style losses from all layers
        weight_per_style_layer = 1.0 / float(self.num_style_layers)
        for target_style, comb_style in zip(gram_style_features, style_output_features):
            style_score += weight_per_style_layer * self.get_style_loss(comb_style, target_style)

        # Accumulate content losses from all layers
        weight_per_content_layer = 1.0 / float(self.num_content_layers)
        for target_content, comb_content in zip(content_features, content_output_features):
            content_score += weight_per_content_layer * self.get_content_loss(comb_content, target_content)

        style_score *= style_weight
        content_score *= content_weight

        # Get total loss
        loss = style_score + content_score
        return loss, style_score, content_score

    def compute_grads(self, cfg):
        """
        Compute gradients

        Args:
            cfg (dict): Configuration dictionary

        Returns:
            tuple: Loss, gradients
        """
        with tf.GradientTape() as tape:
            all_loss = self.compute_loss(**cfg)

        # Compute gradients wrt input image
        total_loss = all_loss[0]
        return tape.gradient(total_loss, cfg['init_image']), all_loss

    def style_transfer(self, content_path, style_path, num_iterations=1000,
                      content_weight=1e3, style_weight=1e-2):
        """
        Perform neural style transfer

        Args:
            content_path (str): Path to content image
            style_path (str): Path to style image
            num_iterations (int): Number of optimization iterations
            content_weight (float): Weight for content loss
            style_weight (float): Weight for style loss

        Returns:
            tf.Tensor: Stylized image
        """
        # We don't need to (or want to) train any layers of our model, so we set their trainable to false
        model = self.build_model()
        for layer in model.layers:
            layer.trainable = False

        # Get the style and content feature representations
        style_features, content_features = self.get_feature_representations(
            model, content_path, style_path)
        gram_style_features = [self.gram_matrix(style_feature) for style_feature in style_features]

        # Set initial image
        init_image = self.load_img(content_path)
        init_image = tf.Variable(init_image, dtype=tf.float32)

        # Create our optimizer
        opt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)

        # For displaying intermediate images
        iter_count = 1

        # Store our best result
        best_loss, best_img = float('inf'), None

        # Create a nice config
        loss_weights = (style_weight, content_weight)
        cfg = {
            'model': model,
            'loss_weights': loss_weights,
            'init_image': init_image,
            'gram_style_features': gram_style_features,
            'content_features': content_features
        }

        # For displaying
        num_rows = 2
        num_cols = 5
        display_interval = num_iterations // (num_rows * num_cols)
        if display_interval == 0:
            display_interval = 1
        start_time = time.time()
        global_start = time.time()

        imgs = []
        for i in range(num_iterations):
            grads, all_loss = self.compute_grads(cfg)
            loss, style_score, content_score = all_loss
            opt.apply_gradients([(grads, init_image)])

            # Clip the image values to valid range
            clipped = tf.clip_by_value(init_image, 0.0, 1.0)
            init_image.assign(clipped)

            end_time = time.time()

            if loss < best_loss:
                # Update best loss and best image from total loss
                best_loss = loss
                best_img = self.tensor_to_image(init_image)

            if i % display_interval == 0:
                start_time = time.time()

                # Use the .numpy() method to get the concrete numpy array
                plot_img = self.tensor_to_image(init_image)
                imgs.append(plot_img)
                print(f'Iteration: {i}')
                print(f'Total loss: {loss.numpy():.4e}, '
                      f'style loss: {style_score.numpy():.4e}, '
                      f'content loss: {content_score.numpy():.4e}, '
                      f'time: {time.time() - start_time:.4f}s')

        print(f'Total time: {time.time() - global_start:.4f}s')

        plt.figure(figsize=(14, 4))
        for i, img in enumerate(imgs):
            plt.subplot(num_rows, num_cols, i+1)
            plt.imshow(img)
            plt.axis('off')
        plt.tight_layout()
        plt.show()

        return best_img, best_loss

    def tensor_to_image(self, tensor):
        """
        Convert tensor to displayable image

        Args:
            tensor (tf.Tensor): Image tensor

        Returns:
            np.array: Image array
        """
        tensor = tensor * 255
        tensor = tf.cast(tensor, tf.uint8)
        if len(tensor.shape) > 3:
            tensor = tf.squeeze(tensor, axis=0)
        return tensor.numpy()

    def deprocess_img(self, processed_img):
        """
        Deprocess image for display (legacy function, kept for compatibility)

        Args:
            processed_img (np.array): Processed image

        Returns:
            np.array: Deprocessed image
        """
        return self.tensor_to_image(tf.convert_to_tensor(processed_img))

def create_sample_images():
    """
    Create sample images for testing (if no images are available)
    """
    # Create a simple content image (blue square with white circle)
    content_img = np.zeros((400, 400, 3), dtype=np.uint8)
    content_img[:, :] = [100, 150, 200]  # Blue background

    # Add a white circle
    center = (200, 200)
    radius = 100
    y, x = np.ogrid[:400, :400]
    mask = (x - center[0])**2 + (y - center[1])**2 <= radius**2
    content_img[mask] = [255, 255, 255]

    # Create a simple style image (red and yellow stripes)
    style_img = np.zeros((400, 400, 3), dtype=np.uint8)
    for i in range(0, 400, 40):
        if (i // 40) % 2 == 0:
            style_img[i:i+40, :] = [255, 0, 0]  # Red stripes
        else:
            style_img[i:i+40, :] = [255, 255, 0]  # Yellow stripes

    # Save sample images
    Image.fromarray(content_img).save('sample_content.jpg')
    Image.fromarray(style_img).save('sample_style.jpg')

    print("Sample images created: sample_content.jpg and sample_style.jpg")
    return 'sample_content.jpg', 'sample_style.jpg'

def main():
    """
    Main function to demonstrate Neural Style Transfer
    """
    print("="*60)
    print("NEURAL STYLE TRANSFER - CODTECH INTERNSHIP PROJECT")
    print("="*60)

    # Initialize the model
    nst = NeuralStyleTransfer(img_height=400, img_width=400)

    # Create sample images if needed
    print("\n1. Creating sample images...")
    content_path, style_path = create_sample_images()

    # Display original images
    print("\n2. Loading and displaying original images...")
    content_image = nst.load_img(content_path)
    style_image = nst.load_img(style_path)

    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    nst.imshow(content_image, 'Content Image')

    plt.subplot(1, 2, 2)
    nst.imshow(style_image, 'Style Image')

    plt.tight_layout()
    plt.show()

    # Perform style transfer
    print("\n3. Performing Neural Style Transfer...")
    print("This may take several minutes...")

    stylized_image, final_loss = nst.style_transfer(
        content_path=content_path,
        style_path=style_path,
        num_iterations=100,  # Reduced for demo
        content_weight=1e3,
        style_weight=1e-2
    )

    # Display final result
    print("\n4. Displaying final result...")
    plt.figure(figsize=(15, 5))

    plt.subplot(1, 3, 1)
    nst.imshow(content_image, 'Original Content')

    plt.subplot(1, 3, 2)
    nst.imshow(style_image, 'Style Reference')

    plt.subplot(1, 3, 3)
    plt.imshow(stylized_image)
    plt.title('Stylized Result')
    plt.axis('off')

    plt.tight_layout()
    plt.show()

    # Save the result
    result_img = Image.fromarray(stylized_image)
    result_img.save('stylized_result.jpg')
    print(f"\nStylized image saved as 'stylized_result.jpg'")
    print(f"Final loss: {final_loss:.4e}")

    print("\n" + "="*60)
    print("NEURAL STYLE TRANSFER COMPLETED SUCCESSFULLY!")
    print("CODTECH INTERNSHIP PROJECT - CERTIFICATE READY")
    print("="*60)

# Example usage for different style transfer experiments
def advanced_examples():
    """
    Advanced examples with different parameters
    """
    print("\nAdvanced Neural Style Transfer Examples")
    print("-" * 50)

    nst = NeuralStyleTransfer(img_height=512, img_width=512)

    # Example 1: Heavy style influence
    print("\nExample 1: Heavy Style Transfer")
    stylized_heavy, _ = nst.style_transfer(
        content_path='sample_content.jpg',
        style_path='sample_style.jpg',
        num_iterations=200,
        content_weight=1e2,  # Lower content weight
        style_weight=1e-1    # Higher style weight
    )

    # Example 2: Light style influence
    print("\nExample 2: Light Style Transfer")
    stylized_light, _ = nst.style_transfer(
        content_path='sample_content.jpg',
        style_path='sample_style.jpg',
        num_iterations=200,
        content_weight=1e4,  # Higher content weight
        style_weight=1e-3    # Lower style weight
    )

    # Display comparison
    plt.figure(figsize=(15, 10))

    content_img = nst.load_img('sample_content.jpg')
    style_img = nst.load_img('sample_style.jpg')

    plt.subplot(2, 3, 1)
    nst.imshow(content_img, 'Content')

    plt.subplot(2, 3, 2)
    nst.imshow(style_img, 'Style')

    plt.subplot(2, 3, 3)
    plt.imshow(stylized_heavy)
    plt.title('Heavy Style')
    plt.axis('off')

    plt.subplot(2, 3, 4)
    plt.text(0.5, 0.5, 'Original Content', ha='center', va='center', fontsize=16)
    plt.axis('off')

    plt.subplot(2, 3, 5)
    plt.text(0.5, 0.5, 'Style Reference', ha='center', va='center', fontsize=16)
    plt.axis('off')

    plt.subplot(2, 3, 6)
    plt.imshow(stylized_light)
    plt.title('Light Style')
    plt.axis('off')

    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    # Install required packages first
    print("Required packages: tensorflow, matplotlib, pillow, numpy")
    print("Install with: pip install tensorflow matplotlib pillow numpy")
    print("\n" + "="*60)

    # Run main demo
    main()

    # Uncomment to run advanced examples
    # advanced_examples()